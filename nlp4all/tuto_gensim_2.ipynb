{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "agricultural-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjustable-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "homeless-consensus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "        for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pointed-retail",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 11:43:10,964 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-03-22 11:43:10,964 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2021-03-22 11:43:10,965 : INFO : saving Dictionary object under /tmp/deerwester.dict, separately None\n",
      "2021-03-22 11:43:10,966 : INFO : saved /tmp/deerwester.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('/tmp/deerwester.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "anonymous-midwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "described-delaware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sudden-pollution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 11:43:15,026 : INFO : storing corpus in Matrix Market format to /tmp/deerwester.mm\n",
      "2021-03-22 11:43:15,028 : INFO : saving sparse matrix to /tmp/deerwester.mm\n",
      "2021-03-22 11:43:15,029 : INFO : PROGRESS: saving document #0\n",
      "2021-03-22 11:43:15,030 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2021-03-22 11:43:15,032 : INFO : saving MmCorpus index to /tmp/deerwester.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lyric-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import open  # for transparently opening remote files\n",
    "\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open('tuto_db.txt'):\n",
    "            # assume there is only one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hollywood-emerald",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x7fdb7254ddf0>\n",
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory\n",
    "print(corpus_memory_friendly)\n",
    "\n",
    "for vector in corpus_memory_friendly:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "oriental-integration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 12:49:46,071 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-03-22 12:49:46,073 : INFO : built Dictionary(42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 69 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('tuto_db.txt'))\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword]\n",
    "           for stopword in stoplist\n",
    "           if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "dictionary.compactify()\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "interim-reaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 12:54:38,946 : INFO : storing corpus in Matrix Market format to /tmp/corpus.mm\n",
      "2021-03-22 12:54:38,949 : INFO : saving sparse matrix to /tmp/corpus.mm\n",
      "2021-03-22 12:54:38,950 : INFO : PROGRESS: saving document #0\n",
      "2021-03-22 12:54:38,951 : INFO : saved 2x2 matrix, density=25.000% (1/4)\n",
      "2021-03-22 12:54:38,953 : INFO : saving MmCorpus index to /tmp/corpus.mm.index\n",
      "2021-03-22 12:54:38,955 : INFO : loaded corpus index from /tmp/corpus.mm.index\n",
      "2021-03-22 12:54:38,956 : INFO : initializing cython corpus reader from /tmp/corpus.mm\n",
      "2021-03-22 12:54:38,958 : INFO : accepted corpus with 2 documents, 2 features, 1 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(2 documents, 2 features, 1 non-zero entries)\n",
      "[[(1, 0.5)], []]\n"
     ]
    }
   ],
   "source": [
    "## Corpus Formats\n",
    "\n",
    "# save corpus\n",
    "corpus = [[(1, 0.5)], []]\n",
    "corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "\n",
    "# load corpus\n",
    "corpus = corpora.MmCorpus('/tmp/corpus.mm')\n",
    "print(corpus)\n",
    "print(list(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "entertaining-newton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1 2 7 8]\n",
      " [6 4 1 4 2]]\n",
      "[[(0, 4.0), (1, 6.0)], [(0, 1.0), (1, 4.0)], [(0, 2.0), (1, 1.0)], [(0, 7.0), (1, 4.0)], [(0, 8.0), (1, 2.0)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "numpy_matrix = np.random.randint(10, size=[2, 5])\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "\n",
    "print(numpy_matrix)\n",
    "print(list(corpus)) # jpp ca lit les vecteurs en colonne --'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "charged-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "scipy_sparse_matrix = scipy.sparse.random(5,2)\n",
    "corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)\n",
    "scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)\n",
    "\n",
    "\n",
    "print(scipy_csc_matrix)\n",
    "#print(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
